<!DOCTYPE html>
<html>
<head>
<title>README_LONG.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="multi-project-docker-health-monitor---complete-guide">Multi-Project Docker Health Monitor - Complete Guide</h1>
<p>A centralized health monitoring system that watches all Docker containers with healthchecks across multiple projects and sends email alerts when containers become unhealthy.</p>
<p><strong>Quick Start Guide:</strong> See <a href="README.md">README.md</a> for quick reference<br>
<strong>This Document:</strong> Comprehensive guide with detailed explanations and troubleshooting</p>
<hr>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#alert-behavior--retry-logic">Alert Behavior &amp; Retry Logic</a></li>
<li><a href="#per-project-health-monitoring-setup">Per-Project Health Monitoring Setup</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
<li><a href="#maintenance">Maintenance</a></li>
<li><a href="#scaling-considerations">Scaling Considerations</a></li>
<li><a href="#security">Security</a></li>
<li><a href="#faq">FAQ</a></li>
</ul>
<hr>
<h2 id="overview">Overview</h2>
<p>This monitoring system automatically discovers and monitors all Docker containers with healthchecks across multiple projects. When a container becomes unhealthy, it implements intelligent retry logic to filter out transient failures before sending email alerts.</p>
<h3 id="key-capabilities">Key Capabilities</h3>
<ul>
<li><strong>Smart Retry Logic</strong>: Waits and re-checks before alerting (reduces false positives by ~80%)</li>
<li><strong>Parallel Monitoring</strong>: Checks multiple containers simultaneously (scales to 50+ containers)</li>
<li><strong>Project-Aware Alerts</strong>: Automatically includes project context in notifications</li>
<li><strong>Flexible Routing</strong>: Send alerts to different teams based on project</li>
<li><strong>Thread-Safe</strong>: Handles concurrent operations safely</li>
<li><strong>Graceful Shutdown</strong>: Waits for pending checks before stopping</li>
<li><strong>Auto-Discovery</strong>: New containers automatically monitored</li>
<li><strong>Production-Grade</strong>: Exponential backoff, jitter, configurable retry strategies</li>
</ul>
<hr>
<h2 id="features">Features</h2>
<h3 id="intelligent-alert-system">Intelligent Alert System</h3>
<p><strong>Problem it solves</strong>: Traditional healthchecks alert immediately on any failure, causing alert fatigue from transient issues (network blips, brief CPU spikes, momentary DB timeouts).</p>
<p><strong>Our solution</strong>:</p>
<ol>
<li>Container becomes unhealthy at 12:00</li>
<li>Monitor detects change but <strong>waits</strong> (configurable, default 10 minutes)</li>
<li>Re-checks at 12:10</li>
<li>Only sends alert if <strong>still unhealthy</strong></li>
<li>Result: ~80% fewer false positive alerts</li>
</ol>
<h3 id="monitoring-features">Monitoring Features</h3>
<ul>
<li>✅ <strong>Automatic Discovery</strong>: Finds all containers with healthchecks</li>
<li>✅ <strong>State Tracking</strong>: Remembers each container's health history</li>
<li>✅ <strong>Change Detection</strong>: Alerts only on status transitions</li>
<li>✅ <strong>Log Inclusion</strong>: Recent container logs in alert emails</li>
<li>✅ <strong>Project Context</strong>: Extracts project names from Docker Compose labels</li>
<li>✅ <strong>Custom Routing</strong>: Different recipients per project</li>
<li>✅ <strong>Parallel Processing</strong>: Checks containers concurrently</li>
<li>✅ <strong>Prevents Duplicates</strong>: Won't trigger multiple retries for same container</li>
</ul>
<h3 id="alert-types">Alert Types</h3>
<ul>
<li><strong>CRITICAL</strong>: Container unhealthy (after retry confirmation)</li>
<li><strong>WARNING</strong>: Container starting</li>
<li><strong>ERROR</strong>: Container not found/removed</li>
<li><strong>INFO</strong>: Container recovered</li>
</ul>
<hr>
<h2 id="architecture">Architecture</h2>
<h3 id="system-flow">System Flow</h3>
<pre class="hljs"><code><div>Container Health Status
        ↓
Docker Engine (every 30min)
        ↓
Health Monitor (queries every 30sec)
        ↓
Change Detected?
   ↓ YES              ↓ NO
Schedule Retry    Continue Monitoring
   ↓
Wait 10 min (configurable)
   ↓
Re-check Health
   ↓
Still Unhealthy?
   ↓ YES              ↓ NO
Send Alert      Log Recovery, No Alert
</div></code></pre>
<h3 id="components">Components</h3>
<p><strong>1. Health Monitor Script</strong> (<code>docker_health_monitor.py</code>)</p>
<ul>
<li>Queries Docker API every 30 seconds</li>
<li>Tracks container states in memory</li>
<li>Schedules background retry tasks</li>
<li>Sends email alerts via SMTP</li>
</ul>
<p><strong>2. Per-Project Health Files</strong> (<code>/app/logs/health_status.txt</code>)</p>
<ul>
<li>Written by each alert application after every run</li>
<li>Contains &quot;OK&quot; or &quot;ERROR&quot; with timestamp</li>
<li>Read by Docker's HEALTHCHECK command</li>
</ul>
<p><strong>3. Docker Healthchecks</strong> (in each project's Dockerfile)</p>
<ul>
<li>Runs every 30 minutes</li>
<li>Checks if health file exists and is recent</li>
<li>Checks if health file contains &quot;OK&quot; (not &quot;ERROR&quot;)</li>
<li>Reports status to Docker Engine</li>
</ul>
<p><strong>4. ThreadPoolExecutor</strong></p>
<ul>
<li>Manages parallel container checks</li>
<li>Handles background retry tasks</li>
<li>Configurable worker pool (default: 30)</li>
<li>Thread-safe state management</li>
</ul>
<hr>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Python 3.7+</li>
<li>Docker installed and running</li>
<li>Docker Compose projects with healthchecks configured</li>
<li>SMTP server access for sending emails</li>
<li>Root/sudo access for systemd service installation</li>
</ul>
<hr>
<h2 id="installation">Installation</h2>
<h3 id="step-1-create-monitoring-directory">Step 1: Create Monitoring Directory</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Navigate to your master folder containing all alert projects</span>
<span class="hljs-built_in">cd</span> /srv/repos/alerts

<span class="hljs-comment"># Create monitoring directory</span>
mkdir -p _docker_health_monitor/logs
<span class="hljs-built_in">cd</span> _docker_health_monitor
</div></code></pre>
<h3 id="step-2-install-the-monitoring-script">Step 2: Install the Monitoring Script</h3>
<p>Copy <code>docker_health_monitor.py</code> (Script 2 - the polished version with retry logic) to the directory.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Make it executable</span>
chmod +x docker_health_monitor.py
</div></code></pre>
<h3 id="step-3-install-python-dependencies">Step 3: Install Python Dependencies</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Create requirements.txt</span>
cat &gt; requirements.txt &lt;&lt; <span class="hljs-string">'EOF'</span>
docker&gt;=7.0.0
python-decouple&gt;=3.8
EOF

<span class="hljs-comment"># Install dependencies</span>
pip3 install -r requirements.txt --<span class="hljs-built_in">break</span>-system-packages
</div></code></pre>
<h3 id="step-4-configure-environment-variables">Step 4: Configure Environment Variables</h3>
<p>Create <code>.env</code> file in the <code>_docker_health_monitor</code> directory:</p>
<pre class="hljs"><code><div>cat &gt; .env &lt;&lt; <span class="hljs-string">'EOF'</span>
<span class="hljs-comment"># ============================================================================</span>
<span class="hljs-comment"># EMAIL CONFIGURATION</span>
<span class="hljs-comment"># ============================================================================</span>
SMTP_HOST=smtp.gmail.com
SMTP_PORT=465
SMTP_USER=your-email@example.com
SMTP_PASS=your-app-password

<span class="hljs-comment"># ============================================================================</span>
<span class="hljs-comment"># DEFAULT ALERT RECIPIENTS (Required)</span>
<span class="hljs-comment"># ============================================================================</span>
<span class="hljs-comment"># These recipients receive all health alerts by default</span>
HEALTH_CHECK_ALERT_EMAILS=ops-team@example.com,admin@example.com

<span class="hljs-comment"># ============================================================================</span>
<span class="hljs-comment"># PROJECT-SPECIFIC ALERT ROUTING (Optional)</span>
<span class="hljs-comment"># ============================================================================</span>
<span class="hljs-comment"># Route alerts for specific projects to specific teams</span>
<span class="hljs-comment"># Format: container-pattern:email1,email2;pattern2:email3</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Examples:</span>
<span class="hljs-comment">#   - Alerts for containers with "passage-plan" go to maritime team</span>
<span class="hljs-comment">#   - Alerts for containers with "vessel-cert" go to compliance team</span>
<span class="hljs-comment">#   - All others go to default recipients above</span>
<span class="hljs-comment">#</span>
CONTAINER_ALERT_ROUTING=passage-plan:maritime-team@example.com;vessel-cert:compliance-team@example.com;hot-works:safety-team@example.com

<span class="hljs-comment"># ============================================================================</span>
<span class="hljs-comment"># MONITORING CONFIGURATION</span>
<span class="hljs-comment"># ============================================================================</span>
<span class="hljs-comment"># How often to check container health (in seconds)</span>
HEALTH_CHECK_INTERVAL_SEC=30

<span class="hljs-comment"># How long to wait before re-checking an unhealthy container (in minutes)</span>
<span class="hljs-comment"># This filters out transient failures</span>
WAIT_AND_CHECK_AGAIN_MIN=10

<span class="hljs-comment"># Number of log lines to include in alert emails</span>
HEALTH_CHECK_LOG_LINES=50

<span class="hljs-comment"># Server name to identify which server sent the alert</span>
SERVER_NAME=Production Server

<span class="hljs-comment"># ============================================================================</span>
<span class="hljs-comment"># ADVANCED RETRY CONFIGURATION (Optional)</span>
<span class="hljs-comment"># ============================================================================</span>
<span class="hljs-comment"># Enable exponential backoff for persistent failures</span>
HEALTH_USE_BACKOFF=<span class="hljs-literal">false</span>

<span class="hljs-comment"># Backoff multiplier (each retry waits: previous_wait * multiplier)</span>
HEALTH_BACKOFF_MULTIPLIER=2.0

<span class="hljs-comment"># Maximum wait time for backoff (in minutes)</span>
HEALTH_BACKOFF_MAX_MIN=30.0

<span class="hljs-comment"># Maximum retry attempts before alerting</span>
HEALTH_MAX_ATTEMPTS=1

<span class="hljs-comment"># Random jitter added to retry timing (prevents thundering herd, in seconds)</span>
HEALTH_RETRY_JITTER_SEC=5.0

<span class="hljs-comment"># Thread pool size for parallel checks (scales up to 50+ containers)</span>
MONITOR_MAX_WORKERS=30

<span class="hljs-comment"># Log file configuration</span>
MONITOR_LOG_FILE=/srv/repos/alerts/_docker_health_monitor/logs/monitor.log
MONITOR_LOG_MAX_BYTES=10485760  <span class="hljs-comment"># 10MB</span>
MONITOR_LOG_BACKUP_COUNT=5
EOF
</div></code></pre>
<p><strong>Edit the <code>.env</code> file</strong> with your actual values:</p>
<pre class="hljs"><code><div>vim .env
chmod 600 .env  <span class="hljs-comment"># Secure the file</span>
</div></code></pre>
<h3 id="step-5-test-the-monitor">Step 5: Test the Monitor</h3>
<p>Before setting up as a service, test it manually:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Run in foreground to see output</span>
python3 docker_health_monitor.py
</div></code></pre>
<p>You should see output like:</p>
<pre class="hljs"><code><div>2025-12-11 10:00:00 [INFO] ======================================================================
2025-12-11 10:00:00 [INFO] Multi-Project Docker Health Monitor initialized
2025-12-11 10:00:00 [INFO] Server: Production Server
2025-12-11 10:00:00 [INFO] Default alert recipients: ops-team@example.com, admin@example.com
2025-12-11 10:00:00 [INFO] Check interval: 30 seconds
2025-12-11 10:00:00 [INFO] Retry wait (base): 10 minutes
2025-12-11 10:00:00 [INFO] Project-specific routing configured for: passage-plan, vessel-cert
2025-12-11 10:00:00 [INFO] Executor workers: 30
2025-12-11 10:00:00 [INFO] ======================================================================
2025-12-11 10:00:00 [INFO] ▶ MULTI-PROJECT DOCKER HEALTH MONITOR STARTED
2025-12-11 10:00:00 [INFO] ======================================================================
2025-12-11 10:00:05 [INFO] [passage-plan] passage-plan-app: unknown → healthy
2025-12-11 10:00:05 [INFO] [vessel-certificates] vessel-cert-app: unknown → healthy
</div></code></pre>
<p>Press <code>Ctrl+C</code> to stop it.</p>
<h3 id="step-6-set-up-systemd-service">Step 6: Set Up Systemd Service</h3>
<p>Create the systemd service file:</p>
<pre class="hljs"><code><div>sudo vim /etc/systemd/system/docker-health-monitor.service
</div></code></pre>
<p>Paste this content:</p>
<pre class="hljs"><code><div><span class="hljs-section">[Unit]</span>
<span class="hljs-attr">Description</span>=Multi-Project Docker Health Monitor
<span class="hljs-attr">Documentation</span>=https://github.com/your-org/docker-health-monitor
<span class="hljs-attr">After</span>=docker.service
<span class="hljs-attr">Requires</span>=docker.service

<span class="hljs-section">[Service]</span>
<span class="hljs-attr">Type</span>=simple
<span class="hljs-attr">User</span>=prominence
<span class="hljs-attr">Group</span>=prominence
<span class="hljs-attr">WorkingDirectory</span>=/srv/repos/alerts/_docker_health_monitor
<span class="hljs-attr">Environment</span>=<span class="hljs-string">"PATH=/usr/bin:/usr/local/bin"</span>
<span class="hljs-attr">ExecStart</span>=/usr/bin/python3 /srv/repos/alerts/_docker_health_monitor/docker_health_monitor.py
<span class="hljs-attr">Restart</span>=always
<span class="hljs-attr">RestartSec</span>=<span class="hljs-number">10</span>

<span class="hljs-comment"># Security settings</span>
<span class="hljs-attr">NoNewPrivileges</span>=<span class="hljs-literal">true</span>
<span class="hljs-attr">PrivateTmp</span>=<span class="hljs-literal">true</span>

<span class="hljs-section">[Install]</span>
<span class="hljs-attr">WantedBy</span>=multi-user.target
</div></code></pre>
<h3 id="step-7-enable-and-start-the-service">Step 7: Enable and Start the Service</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Reload systemd to recognize the new service</span>
sudo systemctl daemon-reload

<span class="hljs-comment"># Enable the service to start on boot</span>
sudo systemctl <span class="hljs-built_in">enable</span> docker-health-monitor

<span class="hljs-comment"># Start the service</span>
sudo systemctl start docker-health-monitor

<span class="hljs-comment"># Check status</span>
sudo systemctl status docker-health-monitor
</div></code></pre>
<p>You should see:</p>
<pre class="hljs"><code><div>● docker-health-monitor.service - Multi-Project Docker Health Monitor
     Loaded: loaded (/etc/systemd/system/docker-health-monitor.service; enabled)
     Active: active (running) since Wed 2025-12-11 10:00:00 EET; 5s ago
   Main PID: 12345 (python3)
      Tasks: 32 (limit: 9448)
     Memory: 28.5M
        CPU: 156ms
     CGroup: /system.slice/docker-health-monitor.service
             └─12345 /usr/bin/python3 /srv/repos/alerts/_docker_health_monitor/docker_health_monitor.py
</div></code></pre>
<hr>
<h2 id="configuration">Configuration</h2>
<h3 id="environment-variables-reference">Environment Variables Reference</h3>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Required</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>SMTP_HOST</code></td>
<td>Yes</td>
<td>-</td>
<td>SMTP server hostname</td>
</tr>
<tr>
<td><code>SMTP_PORT</code></td>
<td>No</td>
<td>465</td>
<td>SMTP server port (465 for SSL, 587 for TLS)</td>
</tr>
<tr>
<td><code>SMTP_USER</code></td>
<td>Yes</td>
<td>-</td>
<td>SMTP username/email</td>
</tr>
<tr>
<td><code>SMTP_PASS</code></td>
<td>Yes</td>
<td>-</td>
<td>SMTP password/app password</td>
</tr>
<tr>
<td><code>HEALTH_CHECK_ALERT_EMAILS</code></td>
<td>Yes</td>
<td>-</td>
<td>Comma-separated default recipients</td>
</tr>
<tr>
<td><code>CONTAINER_ALERT_ROUTING</code></td>
<td>No</td>
<td>-</td>
<td>Project-specific routing (pattern:emails;pattern2:emails)</td>
</tr>
<tr>
<td><code>HEALTH_CHECK_INTERVAL_SEC</code></td>
<td>No</td>
<td>30</td>
<td>How often to check Docker (in seconds)</td>
</tr>
<tr>
<td><code>WAIT_AND_CHECK_AGAIN_MIN</code></td>
<td>No</td>
<td>10</td>
<td>Retry wait time (in minutes)</td>
</tr>
<tr>
<td><code>HEALTH_CHECK_LOG_LINES</code></td>
<td>No</td>
<td>50</td>
<td>Log lines to include in alerts</td>
</tr>
<tr>
<td><code>SERVER_NAME</code></td>
<td>No</td>
<td>Production</td>
<td>Server identifier for alerts</td>
</tr>
<tr>
<td><code>HEALTH_USE_BACKOFF</code></td>
<td>No</td>
<td>false</td>
<td>Enable exponential backoff</td>
</tr>
<tr>
<td><code>HEALTH_BACKOFF_MULTIPLIER</code></td>
<td>No</td>
<td>2.0</td>
<td>Backoff multiplier (2x, 4x, 8x, etc.)</td>
</tr>
<tr>
<td><code>HEALTH_BACKOFF_MAX_MIN</code></td>
<td>No</td>
<td>30.0</td>
<td>Maximum backoff wait (minutes)</td>
</tr>
<tr>
<td><code>HEALTH_MAX_ATTEMPTS</code></td>
<td>No</td>
<td>1</td>
<td>Retry attempts before alerting</td>
</tr>
<tr>
<td><code>HEALTH_RETRY_JITTER_SEC</code></td>
<td>No</td>
<td>5.0</td>
<td>Random jitter (prevents synchronized retries)</td>
</tr>
<tr>
<td><code>MONITOR_MAX_WORKERS</code></td>
<td>No</td>
<td>30</td>
<td>Thread pool size (parallel checks)</td>
</tr>
</tbody>
</table>
<h3 id="configuration-modes">Configuration Modes</h3>
<h4 id="simple-mode-recommended-for-start">Simple Mode (Recommended for Start)</h4>
<p>Best for: 5-10 containers, stable applications, getting started</p>
<pre class="hljs"><code><div>HEALTH_CHECK_INTERVAL_SEC=30
WAIT_AND_CHECK_AGAIN_MIN=10
HEALTH_USE_BACKOFF=<span class="hljs-literal">false</span>
HEALTH_MAX_ATTEMPTS=1
MONITOR_MAX_WORKERS=30
</div></code></pre>
<p><strong>Behavior:</strong></p>
<ul>
<li>Checks every 30 seconds</li>
<li>Waits 10 minutes before alerting</li>
<li>One retry only</li>
<li>Filters out ~80% of transient failures</li>
</ul>
<h4 id="advanced-mode-with-backoff">Advanced Mode with Backoff</h4>
<p>Best for: 10+ containers, occasionally flapping services, large deployments</p>
<pre class="hljs"><code><div>HEALTH_CHECK_INTERVAL_SEC=30
WAIT_AND_CHECK_AGAIN_MIN=5
HEALTH_USE_BACKOFF=<span class="hljs-literal">true</span>
HEALTH_BACKOFF_MULTIPLIER=2.0
HEALTH_BACKOFF_MAX_MIN=30.0
HEALTH_MAX_ATTEMPTS=3
HEALTH_RETRY_JITTER_SEC=5.0
MONITOR_MAX_WORKERS=30
</div></code></pre>
<p><strong>Behavior:</strong></p>
<ul>
<li>Checks every 30 seconds</li>
<li>First retry: 5 minutes</li>
<li>Second retry: 10 minutes (5 * 2.0)</li>
<li>Third retry: 20 minutes (10 * 2.0)</li>
<li>Alerts after 35 total minutes if still unhealthy</li>
<li>Better for persistent issues vs transient</li>
</ul>
<h4 id="high-performance-mode">High-Performance Mode</h4>
<p>Best for: 20+ containers, need fast detection</p>
<pre class="hljs"><code><div>HEALTH_CHECK_INTERVAL_SEC=30
WAIT_AND_CHECK_AGAIN_MIN=5
HEALTH_USE_BACKOFF=<span class="hljs-literal">false</span>
HEALTH_MAX_ATTEMPTS=1
MONITOR_MAX_WORKERS=50
</div></code></pre>
<p><strong>Behavior:</strong></p>
<ul>
<li>Faster first alert (5 minutes)</li>
<li>More workers for parallel processing</li>
<li>Good for large deployments</li>
</ul>
<h3 id="project-specific-routing">Project-Specific Routing</h3>
<p>Route alerts for specific projects to specific teams:</p>
<pre class="hljs"><code><div>CONTAINER_ALERT_ROUTING=passage-plan:maritime@example.com,ops@example.com;vessel-cert:compliance@example.com;hot-works:safety@example.com,ops@example.com
</div></code></pre>
<p><strong>Syntax:</strong></p>
<ul>
<li>Semicolon (<code>;</code>) separates projects</li>
<li>Colon (<code>:</code>) separates pattern from emails</li>
<li>Comma (<code>,</code>) separates multiple emails for same project</li>
</ul>
<p><strong>Pattern Matching:</strong>
The monitor matches against:</p>
<ol>
<li>Container name (e.g., <code>passage-plan-app</code>)</li>
<li>Docker Compose project label</li>
</ol>
<p><strong>Example:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Container: passage-plan-app</span>
<span class="hljs-comment"># Pattern: passage-plan</span>
<span class="hljs-comment"># Result: Sends to maritime@example.com, ops@example.com</span>

<span class="hljs-comment"># Container: vessel-cert-app</span>
<span class="hljs-comment"># Pattern: vessel-cert</span>
<span class="hljs-comment"># Result: Sends to compliance@example.com</span>

<span class="hljs-comment"># Container: work-permits-app</span>
<span class="hljs-comment"># No pattern match</span>
<span class="hljs-comment"># Result: Sends to default recipients (HEALTH_CHECK_ALERT_EMAILS)</span>
</div></code></pre>
<hr>
<h2 id="usage">Usage</h2>
<h3 id="view-logs">View Logs</h3>
<p><strong>File Logs</strong> (recommended):</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Real-time monitoring</span>
tail -f /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># Last 100 lines</span>
tail -n 100 /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># Search for specific container</span>
grep <span class="hljs-string">"passage-plan-app"</span> /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># View all log files (including rotated)</span>
ls -lh /srv/repos/alerts/_docker_health_monitor/logs/
</div></code></pre>
<p><strong>Systemd Journal</strong> (alternative):</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Real-time systemd logs</span>
sudo journalctl -u docker-health-monitor -f

<span class="hljs-comment"># Last 100 lines</span>
sudo journalctl -u docker-health-monitor -n 100

<span class="hljs-comment"># Since specific time</span>
sudo journalctl -u docker-health-monitor --since <span class="hljs-string">"1 hour ago"</span>
</div></code></pre>
<h3 id="manage-the-service">Manage the Service</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Check status</span>
sudo systemctl status docker-health-monitor

<span class="hljs-comment"># Stop (completes pending checks first)</span>
sudo systemctl stop docker-health-monitor

<span class="hljs-comment"># Start</span>
sudo systemctl start docker-health-monitor

<span class="hljs-comment"># Restart (e.g., after config changes)</span>
sudo systemctl restart docker-health-monitor

<span class="hljs-comment"># Disable (won't start on boot)</span>
sudo systemctl <span class="hljs-built_in">disable</span> docker-health-monitor

<span class="hljs-comment"># Enable (will start on boot)</span>
sudo systemctl <span class="hljs-built_in">enable</span> docker-health-monitor

<span class="hljs-comment"># View recent logs</span>
sudo journalctl -u docker-health-monitor -n 50
</div></code></pre>
<h3 id="view-container-health">View Container Health</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Table view of all containers</span>
docker ps --format <span class="hljs-string">"table {{.Names}}\t{{.Status}}"</span>

<span class="hljs-comment"># Only unhealthy containers</span>
docker ps --filter health=unhealthy

<span class="hljs-comment"># Only healthy containers</span>
docker ps --filter health=healthy

<span class="hljs-comment"># Containers in starting state</span>
docker ps --filter health=starting

<span class="hljs-comment"># Check specific container</span>
docker inspect passage-plan-app --format=<span class="hljs-string">'{{.State.Health.Status}}'</span>

<span class="hljs-comment"># Detailed health info (JSON)</span>
docker inspect passage-plan-app --format=<span class="hljs-string">'{{json .State.Health}}'</span> | jq

<span class="hljs-comment"># Watch health status (updates every 2 seconds)</span>
watch -n 2 <span class="hljs-string">'docker ps --format "table {{.Names}}\t{{.Status}}"'</span>
</div></code></pre>
<h3 id="useful-aliases">Useful Aliases</h3>
<p>Add to your <code>~/.zshrc</code>:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Docker health shortcuts</span>
<span class="hljs-built_in">alias</span> dhealth=<span class="hljs-string">'docker ps --format "table {{.Names}}\t{{.Status}}"'</span>
<span class="hljs-built_in">alias</span> dunhealthy=<span class="hljs-string">'docker ps --filter health=unhealthy'</span>
<span class="hljs-built_in">alias</span> dmonlog=<span class="hljs-string">'tail -f /srv/repos/alerts/_docker_health_monitor/logs/monitor.log'</span>

<span class="hljs-comment"># Then use:</span>
<span class="hljs-comment"># dhealth</span>
<span class="hljs-comment"># dunhealthy</span>
<span class="hljs-comment"># dmonlog</span>
</div></code></pre>
<h3 id="test-alert-emails">Test Alert Emails</h3>
<p>Simulate a container failure:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Method 1: Kill container's main process</span>
docker <span class="hljs-built_in">exec</span> passage-plan-app pkill -9 python
<span class="hljs-comment"># Container becomes unhealthy within 30 minutes</span>
<span class="hljs-comment"># Alert sent within 10-40 minutes (depending on retry config)</span>

<span class="hljs-comment"># Method 2: Break application (e.g., wrong DB password)</span>
<span class="hljs-built_in">cd</span> /srv/repos/alerts/passage_plan
vim .env  <span class="hljs-comment"># Change DB_PASS to wrong value</span>
docker compose restart
<span class="hljs-comment"># Next run will fail, write ERROR to health file</span>
<span class="hljs-comment"># Alert sent after retry period</span>

<span class="hljs-comment"># Method 3: Stop container</span>
docker stop passage-plan-app
<span class="hljs-comment"># Alert sent within 30 seconds (no retry for stopped containers)</span>
</div></code></pre>
<h3 id="update-configuration">Update Configuration</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Edit configuration</span>
<span class="hljs-built_in">cd</span> /srv/repos/alerts/_docker_health_monitor
vim .env

<span class="hljs-comment"># Restart service to apply changes</span>
sudo systemctl restart docker-health-monitor

<span class="hljs-comment"># Verify new config loaded</span>
sudo systemctl status docker-health-monitor
tail -f logs/monitor.log
</div></code></pre>
<hr>
<h2 id="alert-behavior--retry-logic">Alert Behavior &amp; Retry Logic</h2>
<h3 id="how-retry-logic-works">How Retry Logic Works</h3>
<h4 id="simple-mode-default">Simple Mode (Default)</h4>
<pre class="hljs"><code><div>12:00:00 - Container becomes unhealthy
12:00:30 - Monitor detects change
12:00:30 - Schedules retry in 10 minutes
12:10:30 - Re-checks container
         ↓
    Still unhealthy? → Send alert
    Recovered? → Log recovery, no alert
</div></code></pre>
<p><strong>Timeline:</strong></p>
<ul>
<li>Detection: Instant</li>
<li>Wait: 10 minutes (configurable)</li>
<li>Total to alert: ~10 minutes</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>Filters out brief failures (DB connection hiccups, network blips)</li>
<li>Reduces alert fatigue by ~80%</li>
<li>Simple, predictable behavior</li>
</ul>
<h4 id="advanced-mode-with-exponential-backoff">Advanced Mode (With Exponential Backoff)</h4>
<pre class="hljs"><code><div>12:00:00 - Container becomes unhealthy
12:00:30 - Monitor detects change
12:00:30 - Schedules retry #1 in 5 minutes
12:05:30 - Re-check #1: Still unhealthy
12:05:30 - Schedules retry #2 in 10 minutes (5 * 2.0)
12:15:30 - Re-check #2: Still unhealthy
12:15:30 - Schedules retry #3 in 20 minutes (10 * 2.0)
12:35:30 - Re-check #3: Still unhealthy → Send alert
</div></code></pre>
<p><strong>Timeline:</strong></p>
<ul>
<li>First check: 5 minutes</li>
<li>Second check: 15 minutes cumulative</li>
<li>Third check: 35 minutes cumulative</li>
<li>Total to alert: ~35 minutes</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>Handles persistent issues better</li>
<li>Prevents alert storms during extended outages</li>
<li>Useful for containers that sometimes take 15-20 minutes to stabilize</li>
</ul>
<h3 id="retry-scenarios">Retry Scenarios</h3>
<h4 id="scenario-1-transient-failure-brief-network-hiccup">Scenario 1: Transient Failure (Brief Network Hiccup)</h4>
<pre class="hljs"><code><div>12:00 - Container unhealthy (network timeout)
12:01 - Connection restored, container healthy
12:10 - Monitor re-checks → Healthy
Result: No alert sent ✓
</div></code></pre>
<h4 id="scenario-2-persistent-failure-database-down">Scenario 2: Persistent Failure (Database Down)</h4>
<pre class="hljs"><code><div>12:00 - Container unhealthy (DB connection failed)
12:10 - Monitor re-checks → Still unhealthy
12:10 - Alert sent ✓
Result: Operations team notified
</div></code></pre>
<h4 id="scenario-3-intermittent-failure-flapping-service">Scenario 3: Intermittent Failure (Flapping Service)</h4>
<p>With backoff enabled:</p>
<pre class="hljs"><code><div>12:00 - Unhealthy
12:05 - Check #1: Still unhealthy
12:15 - Check #2: Still unhealthy
12:35 - Check #3: Still unhealthy → Alert
Result: Waits through multiple flaps before alerting
</div></code></pre>
<h4 id="scenario-4-container-stoppedremoved">Scenario 4: Container Stopped/Removed</h4>
<pre class="hljs"><code><div>12:00 - Container stops
12:00:30 - Monitor detects → Immediate alert (no retry)
Result: Fast notification for critical failures
</div></code></pre>
<h3 id="thread-safety--duplicate-prevention">Thread Safety &amp; Duplicate Prevention</h3>
<p>The monitor uses sophisticated locking to prevent issues:</p>
<p><strong>Problem prevented:</strong></p>
<pre class="hljs"><code><div>Thread A: Detects unhealthy, schedules retry
Thread B: Detects same unhealthy (30 sec later), tries to schedule retry
Result WITHOUT locks: Double alert sent ✗
Result WITH locks: Second attempt blocked, single alert ✓
</div></code></pre>
<p><strong>Implementation:</strong></p>
<ul>
<li><code>container_states_lock</code>: Protects state dictionary</li>
<li><code>retry_futures_lock</code>: Prevents duplicate retry tasks</li>
<li><code>retry_futures</code> dict: Tracks pending retries</li>
</ul>
<p><strong>User benefit:</strong> You never get duplicate alerts for the same issue.</p>
<hr>
<h2 id="per-project-health-monitoring-setup">Per-Project Health Monitoring Setup</h2>
<p>Each alert project needs to write health status files that Docker's HEALTHCHECK can read.</p>
<h3 id="overview">Overview</h3>
<p><strong>Two-level health monitoring:</strong></p>
<ol>
<li>
<p><strong>Docker HEALTHCHECK</strong> (Dockerfile)</p>
<ul>
<li>Runs every 30 minutes</li>
<li>Checks if <code>/app/logs/health_status.txt</code> exists and is recent</li>
<li>Checks if file contains &quot;OK&quot; (not &quot;ERROR&quot;)</li>
</ul>
</li>
<li>
<p><strong>Application Health Writing</strong> (Python code)</p>
<ul>
<li>Writes &quot;OK&quot; or &quot;ERROR&quot; to health file after each run</li>
<li>Includes timestamp and error message</li>
</ul>
</li>
</ol>
<p><strong>Result:</strong> Docker healthcheck detects both:</p>
<ul>
<li>Container crashes (process died)</li>
<li>Application failures (DB errors, exceptions)</li>
</ul>
<h3 id="projects-that-need-updates">Projects That Need Updates</h3>
<p>Based on your structure:</p>
<p><strong>Need updates (have <code>src/core/base_alert.py</code>):</strong></p>
<ul>
<li>✅ <code>flag_dispensations</code></li>
<li>✅ <code>new_vessel_certificates</code></li>
<li>✅ <code>passage_plan</code></li>
<li>✅ <code>work_permits</code></li>
<li>✅ <code>masters_navigation_audit</code> (also needs scheduler changes)</li>
</ul>
<p><strong>Skip for now (older structure):</strong></p>
<ul>
<li><code>events_alerts</code></li>
<li><code>vessel_attendances</code></li>
</ul>
<h3 id="quick-setup-guide">Quick Setup Guide</h3>
<p>For detailed step-by-step instructions, see the short README. Here's the overview:</p>
<p><strong>Files to modify (all projects):</strong></p>
<ol>
<li><code>src/core/base_alert.py</code> - Add <code>_write_health_status()</code> method</li>
<li><code>Dockerfile</code> - Update HEALTHCHECK command</li>
</ol>
<p><strong>Additional files (only for time-based scheduling):</strong>
3. <code>src/core/scheduler.py</code> - Add health writing after alerts run
4. <code>src/main.py</code> - Pass <code>logs_dir</code> to scheduler</p>
<h3 id="what-gets-detected">What Gets Detected</h3>
<p><strong>Application-Level Failures:</strong></p>
<ul>
<li>✅ Database connection errors</li>
<li>✅ API authentication failures</li>
<li>✅ Query execution errors</li>
<li>✅ Unhandled exceptions</li>
<li>✅ File system errors</li>
<li>✅ Network timeouts</li>
<li>✅ Configuration errors</li>
</ul>
<p><strong>Container-Level Failures:</strong></p>
<ul>
<li>✅ Process crashes</li>
<li>✅ Container stops</li>
<li>✅ Container removed</li>
<li>✅ Out of memory (OOM)</li>
<li>✅ CPU/resource exhaustion</li>
</ul>
<h3 id="timeline-for-detection">Timeline for Detection</h3>
<p><strong>Example: Database password changed (application failure)</strong></p>
<pre class="hljs"><code><div>16:00 - Alert runs successfully, writes &quot;OK&quot;
17:00 - Alert runs, database fails, writes &quot;ERROR&quot;
17:30 - Docker healthcheck #1 fails (ERROR detected) - 1/3
18:00 - Docker healthcheck #2 fails - 2/3
18:30 - Docker healthcheck #3 fails - 3/3 → UNHEALTHY
18:30 - Health monitor detects within 30 seconds
18:31 - Retry scheduled (waits 10 minutes)
18:41 - Re-check: Still unhealthy → Alert sent
</div></code></pre>
<p><strong>Total time:</strong> ~100 minutes from error to alert</p>
<ul>
<li>Next scheduled run: ~60 minutes (worst case)</li>
<li>Three healthcheck failures: ~60 minutes</li>
<li>Retry wait: ~10 minutes</li>
<li>Detection: ~30 seconds</li>
</ul>
<p><strong>For faster detection:</strong></p>
<ul>
<li>Reduce Docker healthcheck interval (30m → 15m)</li>
<li>Reduce retry wait (10m → 5m)</li>
<li>Trade-off: More resource usage, more false positives</li>
</ul>
<hr>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="monitor-not-starting">Monitor Not Starting</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Check service status</span>
sudo systemctl status docker-health-monitor

<span class="hljs-comment"># View recent errors</span>
sudo journalctl -u docker-health-monitor -n 50

<span class="hljs-comment"># Check file logs</span>
tail -n 50 /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># Run manually to see errors</span>
<span class="hljs-built_in">cd</span> /srv/repos/alerts/_docker_health_monitor
python3 docker_health_monitor.py
</div></code></pre>
<p><strong>Common issues:</strong></p>
<ul>
<li>Missing <code>.env</code> file → Create it</li>
<li>Wrong file permissions → <code>chmod 600 .env</code></li>
<li>Missing Python packages → <code>pip3 install -r requirements.txt --break-system-packages</code></li>
<li>No Docker access → <code>sudo usermod -aG docker prominence</code></li>
</ul>
<h3 id="no-alerts-being-sent">No Alerts Being Sent</h3>
<p><strong>1. Check SMTP configuration:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Test SMTP manually</span>
python3 -c <span class="hljs-string">"
import smtplib
from decouple import config
server = smtplib.SMTP_SSL(config('SMTP_HOST'), int(config('SMTP_PORT')))
server.login(config('SMTP_USER'), config('SMTP_PASS'))
print('✓ SMTP connection successful')
server.quit()
"</span>
</div></code></pre>
<p>If this fails:</p>
<ul>
<li>Gmail: Enable 2FA and create App Password</li>
<li>Office 365: Use app-specific password</li>
<li>Custom SMTP: Check firewall/ports</li>
</ul>
<p><strong>2. Verify containers have healthchecks:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># List containers with health status</span>
docker ps --format <span class="hljs-string">"table {{.Names}}\t{{.Status}}"</span>

<span class="hljs-comment"># Containers without "(healthy)" aren't monitored</span>
<span class="hljs-comment"># They need HEALTHCHECK in Dockerfile</span>
</div></code></pre>
<p><strong>3. Check monitor logs:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Look for "retry" messages</span>
grep <span class="hljs-string">"retry"</span> /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># Look for "still.*unhealthy"</span>
grep <span class="hljs-string">"still.*unhealthy"</span> /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># Look for "Alert sent"</span>
grep <span class="hljs-string">"Alert sent"</span> /srv/repos/alerts/_docker_health_monitor/logs/monitor.log
</div></code></pre>
<p><strong>4. Verify retry timing:</strong></p>
<p>Are you waiting long enough? Default retry is 10 minutes.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Check your config</span>
grep WAIT_AND_CHECK_AGAIN /srv/repos/alerts/_docker_health_monitor/.env

<span class="hljs-comment"># If container became unhealthy at 12:00</span>
<span class="hljs-comment"># First retry happens at 12:10</span>
<span class="hljs-comment"># Alert sent at 12:10 if still unhealthy</span>
</div></code></pre>
<h3 id="containers-not-being-monitored">Containers Not Being Monitored</h3>
<p>The monitor only watches containers with healthchecks.</p>
<p><strong>Check if container has healthcheck:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Method 1: ps output</span>
docker ps --format <span class="hljs-string">"table {{.Names}}\t{{.Status}}"</span>
<span class="hljs-comment"># Look for "(healthy)" or "(unhealthy)" in status</span>

<span class="hljs-comment"># Method 2: inspect</span>
docker inspect passage-plan-app --format=<span class="hljs-string">'{{.State.Health.Status}}'</span>
<span class="hljs-comment"># If blank/null → no healthcheck configured</span>
</div></code></pre>
<p><strong>Add healthcheck to Dockerfile:</strong></p>
<pre class="hljs"><code><div><span class="hljs-keyword">HEALTHCHECK</span><span class="bash"> --interval=30m --timeout=10s --start-period=30s --retries=3 \
  CMD <span class="hljs-built_in">test</span> -f /app/logs/health_status.txt &amp;&amp; \
      MINUTES=$(python3 -c <span class="hljs-string">"import os; schedule_times = os.getenv('SCHEDULE_TIMES', ''); print(1440 if schedule_times else int(float(os.getenv('SCHEDULE_FREQUENCY_HOURS', '1')) * 60 + 10))"</span>) &amp;&amp; \
      <span class="hljs-built_in">test</span> $(find /app/logs/health_status.txt -mmin -<span class="hljs-variable">${MINUTES}</span> | wc -l) -eq 1 &amp;&amp; \
      grep -q <span class="hljs-string">"^OK"</span> /app/logs/health_status.txt || <span class="hljs-built_in">exit</span> 1</span>
</div></code></pre>
<p>Then rebuild:</p>
<pre class="hljs"><code><div>docker compose build --no-cache &amp;&amp; docker compose up -d
</div></code></pre>
<h3 id="alerts-going-to-wrong-recipients">Alerts Going to Wrong Recipients</h3>
<p><strong>Check routing configuration:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># View current routing</span>
grep CONTAINER_ALERT_ROUTING /srv/repos/alerts/_docker_health_monitor/.env

<span class="hljs-comment"># Check monitor logs to see which routing was applied</span>
grep <span class="hljs-string">"Using project-specific routing"</span> /srv/repos/alerts/_docker_health_monitor/logs/monitor.log
</div></code></pre>
<p><strong>Pattern matching rules:</strong></p>
<ul>
<li>Matches container name OR project name</li>
<li>First matching pattern wins</li>
<li>If no match, uses default recipients</li>
</ul>
<p><strong>Example debugging:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Container: passage-plan-app</span>
<span class="hljs-comment"># Routing: passage-plan:maritime@ex.com;plan:ops@ex.com</span>

<span class="hljs-comment"># Which pattern matches?</span>
<span class="hljs-comment"># Answer: "passage-plan" (first match)</span>
<span class="hljs-comment"># Recipients: maritime@ex.com</span>
</div></code></pre>
<h3 id="high-memorycpu-usage">High Memory/CPU Usage</h3>
<p><strong>Check resource usage:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Monitor stats</span>
docker stats docker-health-monitor

<span class="hljs-comment"># Or if running as systemd service</span>
systemctl status docker-health-monitor
<span class="hljs-comment"># Look at Memory/CPU lines</span>
</div></code></pre>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Increase check interval:</strong></li>
</ol>
<pre class="hljs"><code><div>   <span class="hljs-comment"># In .env</span>
   HEALTH_CHECK_INTERVAL_SEC=60  <span class="hljs-comment"># Was 30</span>
</div></code></pre>
<ol start="2">
<li><strong>Reduce worker threads:</strong></li>
</ol>
<pre class="hljs"><code><div>   <span class="hljs-comment"># In .env</span>
   MONITOR_MAX_WORKERS=10  <span class="hljs-comment"># Was 30</span>
</div></code></pre>
<ol start="3">
<li><strong>Disable backoff if enabled:</strong></li>
</ol>
<pre class="hljs"><code><div>   <span class="hljs-comment"># In .env</span>
   HEALTH_USE_BACKOFF=<span class="hljs-literal">false</span>
</div></code></pre>
<p><strong>Expected resource usage:</strong></p>
<ul>
<li>Memory: 20-50MB (depends on container count)</li>
<li>CPU: &lt;1% when idle, 2-5% during checks</li>
<li>If much higher: Check for stuck threads or excessive logging</li>
</ul>
<h3 id="container-shows-unhealthy-but-application-works">Container Shows Unhealthy But Application Works</h3>
<p><strong>Possible causes:</strong></p>
<ol>
<li><strong>Health file not being written:</strong></li>
</ol>
<pre class="hljs"><code><div>   <span class="hljs-comment"># Check if file exists</span>
   docker <span class="hljs-built_in">exec</span> &lt;container&gt; cat /app/logs/health_status.txt
   
   <span class="hljs-comment"># If missing: Application health monitoring not configured</span>
   <span class="hljs-comment"># See "Per-Project Health Monitoring Setup" section</span>
</div></code></pre>
<ol start="2">
<li><strong>Health file too old:</strong></li>
</ol>
<pre class="hljs"><code><div>   <span class="hljs-comment"># Check file timestamp</span>
   docker <span class="hljs-built_in">exec</span> &lt;container&gt; <span class="hljs-built_in">stat</span> /app/logs/health_status.txt
   
   <span class="hljs-comment"># If old: Application not running on schedule</span>
   <span class="hljs-comment"># Check application logs</span>
   docker logs &lt;container&gt; --tail 100
</div></code></pre>
<ol start="3">
<li><strong>Health file contains ERROR:</strong></li>
</ol>
<pre class="hljs"><code><div>   docker <span class="hljs-built_in">exec</span> &lt;container&gt; cat /app/logs/health_status.txt
   
   <span class="hljs-comment"># If "ERROR": Check error message in file</span>
   <span class="hljs-comment"># Fix underlying issue (DB, API, etc.)</span>
</div></code></pre>
<ol start="4">
<li><strong>HEALTHCHECK misconfigured:</strong></li>
</ol>
<pre class="hljs"><code><div>   <span class="hljs-comment"># Test healthcheck manually</span>
   docker <span class="hljs-built_in">exec</span> &lt;container&gt; sh -c <span class="hljs-string">'test -f /app/logs/health_status.txt &amp;&amp; grep -q "^OK" /app/logs/health_status.txt &amp;&amp; echo "PASS" || echo "FAIL"'</span>
   
   <span class="hljs-comment"># If FAIL: Check Dockerfile HEALTHCHECK command</span>
</div></code></pre>
<h3 id="getting-too-many-alerts-false-positives">Getting Too Many Alerts (False Positives)</h3>
<p><strong>Increase retry wait time:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># In .env</span>
WAIT_AND_CHECK_AGAIN_MIN=15  <span class="hljs-comment"># Was 10</span>

<span class="hljs-comment"># Or enable backoff</span>
HEALTH_USE_BACKOFF=<span class="hljs-literal">true</span>
HEALTH_MAX_ATTEMPTS=2
</div></code></pre>
<p><strong>Or adjust Docker healthcheck:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># In Dockerfile, increase retries</span>
<span class="hljs-keyword">HEALTHCHECK</span><span class="bash"> --interval=30m --timeout=10s --start-period=30s --retries=5 \
  <span class="hljs-comment"># ... rest of command</span></span>
</div></code></pre>
<p>More retries = more tolerance for transient failures.</p>
<h3 id="getting-too-few-alerts-missing-real-issues">Getting Too Few Alerts (Missing Real Issues)</h3>
<p><strong>Reduce retry wait time:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># In .env</span>
WAIT_AND_CHECK_AGAIN_MIN=5  <span class="hljs-comment"># Was 10</span>

<span class="hljs-comment"># Disable backoff if enabled</span>
HEALTH_USE_BACKOFF=<span class="hljs-literal">false</span>
</div></code></pre>
<p><strong>Or reduce Docker healthcheck interval:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># In Dockerfile, check more frequently</span>
<span class="hljs-keyword">HEALTHCHECK</span><span class="bash"> --interval=15m --timeout=10s --start-period=30s --retries=3 \
  <span class="hljs-comment"># ... rest of command</span></span>
</div></code></pre>
<p><strong>Trade-off:</strong> Faster detection vs more false positives.</p>
<h3 id="retry-tasks-not-working">Retry Tasks Not Working</h3>
<p><strong>Check executor status:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Look for executor errors in logs</span>
grep -i <span class="hljs-string">"executor"</span> /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># Look for retry scheduling</span>
grep <span class="hljs-string">"scheduling retry"</span> /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># Look for duplicate prevention</span>
grep <span class="hljs-string">"already scheduled"</span> /srv/repos/alerts/_docker_health_monitor/logs/monitor.log
</div></code></pre>
<p><strong>Verify configuration:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Check retry settings</span>
grep -E <span class="hljs-string">"(WAIT_AND_CHECK|HEALTH_USE_BACKOFF|HEALTH_MAX_ATTEMPTS)"</span> /srv/repos/alerts/_docker_health_monitor/.env
</div></code></pre>
<p><strong>Restart service if stuck:</strong></p>
<pre class="hljs"><code><div>sudo systemctl restart docker-health-monitor
</div></code></pre>
<hr>
<h2 id="maintenance">Maintenance</h2>
<h3 id="log-rotation">Log Rotation</h3>
<p><strong>Automatic rotation</strong> (built-in):</p>
<ul>
<li>Log file rotates at 10MB</li>
<li>Keeps 5 backup files</li>
<li>Total disk usage: &lt;60MB</li>
<li>No manual intervention needed</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-comment"># View all log files</span>
ls -lh /srv/repos/alerts/_docker_health_monitor/logs/
<span class="hljs-comment"># Output:</span>
<span class="hljs-comment"># monitor.log      (current, up to 10MB)</span>
<span class="hljs-comment"># monitor.log.1    (most recent backup)</span>
<span class="hljs-comment"># monitor.log.2</span>
<span class="hljs-comment"># monitor.log.3</span>
<span class="hljs-comment"># monitor.log.4</span>
<span class="hljs-comment"># monitor.log.5    (oldest, will be deleted on next rotation)</span>

<span class="hljs-comment"># Total disk usage</span>
du -sh /srv/repos/alerts/_docker_health_monitor/logs/
</div></code></pre>
<p><strong>Manual cleanup</strong> (if needed):</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Remove old backups</span>
rm /srv/repos/alerts/_docker_health_monitor/logs/monitor.log.[1-5]

<span class="hljs-comment"># Clear current log (start fresh)</span>
&gt; /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># Restart service after manual cleanup</span>
sudo systemctl restart docker-health-monitor
</div></code></pre>
<p><strong>Systemd journal logs:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># View journal disk usage</span>
sudo journalctl --disk-usage

<span class="hljs-comment"># Clean old journals (older than 2 weeks)</span>
sudo journalctl --vacuum-time=2weeks

<span class="hljs-comment"># Limit journal size (500MB max)</span>
sudo journalctl --vacuum-size=500M
</div></code></pre>
<h3 id="backup-configuration">Backup Configuration</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Backup .env with timestamp</span>
cp /srv/repos/alerts/_docker_health_monitor/.env \
   /srv/repos/alerts/_docker_health_monitor/.env.$(date +%Y%m%d_%H%M%S)

<span class="hljs-comment"># Backup logs directory</span>
tar -czf /tmp/monitor-logs-$(date +%Y%m%d).tar.gz \
   /srv/repos/alerts/_docker_health_monitor/logs/

<span class="hljs-comment"># Backup entire monitor directory</span>
tar -czf /tmp/docker-health-monitor-backup-$(date +%Y%m%d).tar.gz \
   /srv/repos/alerts/_docker_health_monitor/ \
   --exclude=<span class="hljs-string">'logs'</span> \
   --exclude=<span class="hljs-string">'*.pyc'</span>
</div></code></pre>
<h3 id="updates">Updates</h3>
<p><strong>Update Python script:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Stop service</span>
sudo systemctl stop docker-health-monitor

<span class="hljs-comment"># Backup current version</span>
cp /srv/repos/alerts/_docker_health_monitor/docker_health_monitor.py \
   /srv/repos/alerts/_docker_health_monitor/docker_health_monitor.py.backup

<span class="hljs-comment"># Copy new version</span>
<span class="hljs-comment"># ... copy new docker_health_monitor.py ...</span>

<span class="hljs-comment"># Update dependencies if needed</span>
pip3 install -r requirements.txt --upgrade --<span class="hljs-built_in">break</span>-system-packages

<span class="hljs-comment"># Start service</span>
sudo systemctl start docker-health-monitor

<span class="hljs-comment"># Verify</span>
sudo systemctl status docker-health-monitor
tail -f /srv/repos/alerts/_docker_health_monitor/logs/monitor.log
</div></code></pre>
<p><strong>Update configuration:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Edit .env</span>
vim /srv/repos/alerts/_docker_health_monitor/.env

<span class="hljs-comment"># Restart to apply</span>
sudo systemctl restart docker-health-monitor
</div></code></pre>
<h3 id="databasestate-management">Database/State Management</h3>
<p>The monitor is <strong>stateless</strong> except for:</p>
<ul>
<li>In-memory container states (lost on restart - this is OK)</li>
<li>Pending retry tasks (cancelled on restart - this is OK)</li>
</ul>
<p><strong>What happens on restart:</strong></p>
<pre class="hljs"><code><div>1. Service stops
2. Executor waits for pending retries to finish (graceful)
3. In-memory state cleared
4. Service starts
5. Re-discovers all containers
6. Marks all as &quot;unknown&quot; → current state
7. Only alerts on future changes
</div></code></pre>
<p><strong>Result:</strong> No false alerts on restart.</p>
<hr>
<h2 id="scaling-considerations">Scaling Considerations</h2>
<h3 id="current-deployment-5-7-containers">Current Deployment (5-7 Containers)</h3>
<p><strong>Configuration:</strong></p>
<pre class="hljs"><code><div>MONITOR_MAX_WORKERS=30
HEALTH_CHECK_INTERVAL_SEC=30
WAIT_AND_CHECK_AGAIN_MIN=10
</div></code></pre>
<p><strong>Performance:</strong></p>
<ul>
<li>Parallel checks: 30 workers available, only using 5-7</li>
<li>Check time: ~2-3 seconds per cycle (network latency)</li>
<li>CPU usage: &lt;1%</li>
<li>Memory: ~25MB</li>
<li>Plenty of headroom</li>
</ul>
<h3 id="medium-deployment-15-25-containers">Medium Deployment (15-25 Containers)</h3>
<p><strong>Recommended configuration:</strong></p>
<pre class="hljs"><code><div>MONITOR_MAX_WORKERS=30  <span class="hljs-comment"># No change needed</span>
HEALTH_CHECK_INTERVAL_SEC=30
WAIT_AND_CHECK_AGAIN_MIN=10  <span class="hljs-comment"># Or reduce to 5 for faster alerts</span>
HEALTH_USE_BACKOFF=<span class="hljs-literal">false</span>  <span class="hljs-comment"># Or enable if services flap</span>
</div></code></pre>
<p><strong>Expected performance:</strong></p>
<ul>
<li>Parallel checks: All 25 checked simultaneously</li>
<li>Check time: Still ~2-3 seconds per cycle</li>
<li>CPU usage: 1-2%</li>
<li>Memory: ~35MB</li>
<li>No performance degradation</li>
</ul>
<h3 id="large-deployment-50-containers">Large Deployment (50+ Containers)</h3>
<p><strong>Recommended configuration:</strong></p>
<pre class="hljs"><code><div>MONITOR_MAX_WORKERS=50  <span class="hljs-comment"># Increase workers</span>
HEALTH_CHECK_INTERVAL_SEC=30
WAIT_AND_CHECK_AGAIN_MIN=5  <span class="hljs-comment"># Faster initial check</span>
HEALTH_USE_BACKOFF=<span class="hljs-literal">true</span>  <span class="hljs-comment"># Better for scale</span>
HEALTH_MAX_ATTEMPTS=3
</div></code></pre>
<p><strong>Expected performance:</strong></p>
<ul>
<li>Parallel checks: 50 containers simultaneously</li>
<li>Check time: Still ~2-3 seconds per cycle</li>
<li>CPU usage: 2-5%</li>
<li>Memory: ~50-70MB</li>
<li>Scales linearly</li>
</ul>
<h3 id="very-large-deployment-100-containers">Very Large Deployment (100+ Containers)</h3>
<p>At this scale, consider:</p>
<ol>
<li><strong>Multiple monitor instances</strong> (shard by project):</li>
</ol>
<pre class="hljs"><code><div>   <span class="hljs-comment"># Monitor A: Projects 1-50</span>
   <span class="hljs-comment"># Monitor B: Projects 51-100</span>
</div></code></pre>
<ol start="2">
<li><strong>Increase resources:</strong></li>
</ol>
<pre class="hljs"><code><div>   MONITOR_MAX_WORKERS=100
   HEALTH_CHECK_INTERVAL_SEC=60  <span class="hljs-comment"># Less frequent checks</span>
</div></code></pre>
<ol start="3">
<li><strong>Database-backed state</strong> (requires code modification):
<ul>
<li>Store state in Redis/PostgreSQL</li>
<li>Share state across multiple monitors</li>
<li>Persist state across restarts</li>
</ul>
</li>
</ol>
<h3 id="performance-characteristics">Performance Characteristics</h3>
<p><strong>Sequential (Script 1) vs Parallel (Script 2):</strong></p>
<table>
<thead>
<tr>
<th>Containers</th>
<th>Script 1 (Sequential)</th>
<th>Script 2 (Parallel)</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>~10 seconds</td>
<td>~3 seconds</td>
</tr>
<tr>
<td>10</td>
<td>~20 seconds</td>
<td>~3 seconds</td>
</tr>
<tr>
<td>25</td>
<td>~50 seconds</td>
<td>~3 seconds</td>
</tr>
<tr>
<td>50</td>
<td>~100 seconds</td>
<td>~3 seconds</td>
</tr>
<tr>
<td>100</td>
<td>~200 seconds</td>
<td>~5 seconds</td>
</tr>
</tbody>
</table>
<p><strong>Key insight:</strong> Script 2's parallel processing means check time is constant regardless of container count (up to worker limit).</p>
<p><strong>Bottleneck:</strong> Network/Docker API latency (~2-3 seconds), not code.</p>
<h3 id="resource-usage-scaling">Resource Usage Scaling</h3>
<table>
<thead>
<tr>
<th>Containers</th>
<th>Memory</th>
<th>CPU (check)</th>
<th>CPU (idle)</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>25MB</td>
<td>0.5%</td>
<td>&lt;0.1%</td>
</tr>
<tr>
<td>10</td>
<td>30MB</td>
<td>1%</td>
<td>&lt;0.1%</td>
</tr>
<tr>
<td>25</td>
<td>40MB</td>
<td>2%</td>
<td>&lt;0.1%</td>
</tr>
<tr>
<td>50</td>
<td>60MB</td>
<td>3-5%</td>
<td>&lt;0.1%</td>
</tr>
<tr>
<td>100</td>
<td>90MB</td>
<td>5-10%</td>
<td>0.1%</td>
</tr>
</tbody>
</table>
<p><strong>Linear scaling:</strong> Each container adds ~0.5MB memory, 0.05% CPU.</p>
<hr>
<h2 id="security">Security</h2>
<h3 id="smtp-credentials">SMTP Credentials</h3>
<p><strong>The <code>.env</code> file contains sensitive credentials.</strong></p>
<p><strong>Protect it:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Set restrictive permissions</span>
chmod 600 /srv/repos/alerts/_docker_health_monitor/.env
chown prominence:prominence /srv/repos/alerts/_docker_health_monitor/.env

<span class="hljs-comment"># Verify</span>
ls -la /srv/repos/alerts/_docker_health_monitor/.env
<span class="hljs-comment"># Should show: -rw------- 1 prominence prominence</span>
</div></code></pre>
<p><strong>Don't commit to git:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Add to .gitignore</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">".env"</span> &gt;&gt; .gitignore
</div></code></pre>
<p><strong>Use app-specific passwords:</strong></p>
<ul>
<li>Gmail: Create App Password (not account password)</li>
<li>Office 365: Use app-specific password</li>
<li>Never use your main email password</li>
</ul>
<h3 id="docker-socket-access">Docker Socket Access</h3>
<p><strong>The monitor needs read access to <code>/var/run/docker.sock</code>.</strong></p>
<p><strong>Ensure user has docker group:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Check current groups</span>
groups prominence

<span class="hljs-comment"># Add to docker group if needed</span>
sudo usermod -aG docker prominence

<span class="hljs-comment"># Log out and back in for changes to take effect</span>
</div></code></pre>
<p><strong>Why this is safe:</strong></p>
<ul>
<li>Monitor only reads container state</li>
<li>Doesn't start/stop/modify containers</li>
<li>Read-only operations via Docker API</li>
</ul>
<h3 id="systemd-service-isolation">Systemd Service Isolation</h3>
<p><strong>Security settings in service file:</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Prevents privilege escalation</span>
<span class="hljs-attr">NoNewPrivileges</span>=<span class="hljs-literal">true</span>

<span class="hljs-comment"># Isolates /tmp directory</span>
<span class="hljs-attr">PrivateTmp</span>=<span class="hljs-literal">true</span>
</div></code></pre>
<p><strong>Additional hardening (optional):</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Add to [Service] section</span>
<span class="hljs-attr">ProtectSystem</span>=strict
<span class="hljs-attr">ProtectHome</span>=<span class="hljs-literal">true</span>
<span class="hljs-attr">ReadWritePaths</span>=/srv/repos/alerts/_docker_health_monitor/logs
<span class="hljs-attr">PrivateDevices</span>=<span class="hljs-literal">true</span>
<span class="hljs-attr">ProtectKernelTunables</span>=<span class="hljs-literal">true</span>
<span class="hljs-attr">ProtectControlGroups</span>=<span class="hljs-literal">true</span>
<span class="hljs-attr">RestrictRealtime</span>=<span class="hljs-literal">true</span>
</div></code></pre>
<p>Apply with:</p>
<pre class="hljs"><code><div>sudo systemctl daemon-reload
sudo systemctl restart docker-health-monitor
</div></code></pre>
<h3 id="network-security">Network Security</h3>
<p><strong>SMTP traffic:</strong></p>
<ul>
<li>Uses SSL/TLS (port 465) by default</li>
<li>Credentials encrypted in transit</li>
<li>Configure firewall if needed:</li>
</ul>
<pre class="hljs"><code><div>  <span class="hljs-comment"># Allow outbound SMTP</span>
  sudo ufw allow out 465/tcp
</div></code></pre>
<p><strong>Docker API:</strong></p>
<ul>
<li>Unix socket (local only)</li>
<li>No network exposure</li>
<li>No authentication needed (local socket)</li>
</ul>
<hr>
<h2 id="faq">FAQ</h2>
<h3 id="q-do-i-need-to-restart-the-monitor-when-i-addremove-containers">Q: Do I need to restart the monitor when I add/remove containers?</h3>
<p><strong>A:</strong> No. The monitor auto-discovers containers every 30 seconds.</p>
<pre class="hljs"><code><div>12:00:00 - Monitor running, sees 5 containers
12:05:30 - Deploy new container (6 total)
12:05:45 - Monitor discovers new container automatically
12:05:45 - Starts monitoring new container
</div></code></pre>
<h3 id="q-what-happens-if-the-monitor-crashes">Q: What happens if the monitor crashes?</h3>
<p><strong>A:</strong> Systemd automatically restarts it (RestartSec=10).</p>
<pre class="hljs"><code><div>12:00:00 - Monitor crashes
12:00:10 - Systemd restarts monitor
12:00:15 - Monitor re-discovers all containers
12:00:15 - Resumes monitoring
</div></code></pre>
<p><strong>Impact:</strong> 10-15 seconds of missed checks (acceptable).</p>
<h3 id="q-can-i-run-multiple-monitors">Q: Can I run multiple monitors?</h3>
<p><strong>A:</strong> Yes, but not recommended unless you have 100+ containers.</p>
<p><strong>If you do:</strong></p>
<ul>
<li>Each monitor should watch different projects</li>
<li>Configure different routing for each</li>
<li>Use different log files</li>
</ul>
<p><strong>Example setup:</strong></p>
<pre class="hljs"><code><div>Monitor A: Projects 1-50, logs to monitor-a.log
Monitor B: Projects 51-100, logs to monitor-b.log
</div></code></pre>
<h3 id="q-does-the-monitor-store-any-persistent-data">Q: Does the monitor store any persistent data?</h3>
<p><strong>A:</strong> No. All state is in-memory and lost on restart.</p>
<p><strong>This is by design:</strong></p>
<ul>
<li>Simplicity</li>
<li>No state corruption</li>
<li>Clean startup each time</li>
<li>No database needed</li>
</ul>
<p><strong>Implication:</strong> Alerts based on state changes only work for changes <em>after</em> monitor starts.</p>
<h3 id="q-how-do-i-know-the-monitor-is-working">Q: How do I know the monitor is working?</h3>
<p><strong>Check 1: Service status</strong></p>
<pre class="hljs"><code><div>sudo systemctl status docker-health-monitor
<span class="hljs-comment"># Should show: active (running)</span>
</div></code></pre>
<p><strong>Check 2: Recent log activity</strong></p>
<pre class="hljs"><code><div>tail -n 20 /srv/repos/alerts/_docker_health_monitor/logs/monitor.log
<span class="hljs-comment"># Should show container checks every 30 seconds</span>
</div></code></pre>
<p><strong>Check 3: Container discovery</strong></p>
<pre class="hljs"><code><div>grep <span class="hljs-string">"Monitoring.*container"</span> /srv/repos/alerts/_docker_health_monitor/logs/monitor.log | tail -1
<span class="hljs-comment"># Should show correct container count</span>
</div></code></pre>
<p><strong>Check 4: Test alert</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Stop a container</span>
docker stop <span class="hljs-built_in">test</span>-container

<span class="hljs-comment"># Check logs within 30 seconds</span>
tail -f /srv/repos/alerts/_docker_health_monitor/logs/monitor.log
<span class="hljs-comment"># Should show: "Container no longer running"</span>
</div></code></pre>
<h3 id="q-can-i-monitor-containers-without-healthchecks">Q: Can I monitor containers without healthchecks?</h3>
<p><strong>A:</strong> No. The monitor only watches containers with <code>HEALTHCHECK</code> configured.</p>
<p><strong>Why:</strong> Without healthchecks, Docker doesn't track container health. The monitor has no signal to monitor.</p>
<p><strong>Solution:</strong> Add HEALTHCHECK to Dockerfile (see &quot;Per-Project Health Monitoring Setup&quot;).</p>
<h3 id="q-whats-the-difference-between-dockers-healthcheck-and-this-monitor">Q: What's the difference between Docker's healthcheck and this monitor?</h3>
<p><strong>Docker's HEALTHCHECK:</strong></p>
<ul>
<li>Runs inside container</li>
<li>Checks application health</li>
<li>Updates container status (healthy/unhealthy)</li>
<li>No external notifications</li>
</ul>
<p><strong>This Monitor:</strong></p>
<ul>
<li>Runs outside containers</li>
<li>Watches Docker's health status</li>
<li>Sends email alerts on changes</li>
<li>Implements retry logic</li>
</ul>
<p><strong>Together:</strong> Docker detects issues, monitor notifies you.</p>
<h3 id="q-can-i-customize-the-alert-email-format">Q: Can I customize the alert email format?</h3>
<p><strong>A:</strong> Yes, but requires code modification.</p>
<p><strong>Current:</strong> Emails are formatted in <code>send_alert_email()</code> method.</p>
<p><strong>To customize:</strong></p>
<ol>
<li>Edit <code>docker_health_monitor.py</code></li>
<li>Find <code>send_alert_email()</code> method</li>
<li>Modify <code>body</code> variable</li>
<li>Restart service</li>
</ol>
<p><strong>Alternative:</strong> Use project-specific routing to different ticketing systems (ServiceNow, Jira, PagerDuty) via email.</p>
<h3 id="q-how-do-i-test-my-retry-configuration">Q: How do I test my retry configuration?</h3>
<p><strong>Method 1: Break a container temporarily</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># Break DB connection</span>
vim /srv/repos/alerts/passage_plan/.env
<span class="hljs-comment"># Change DB_PASS to wrong value</span>
docker compose restart

<span class="hljs-comment"># Watch logs</span>
tail -f /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># You'll see:</span>
<span class="hljs-comment"># - Initial detection</span>
<span class="hljs-comment"># - Retry scheduled</span>
<span class="hljs-comment"># - Re-check after wait period</span>
<span class="hljs-comment"># - Alert sent if still unhealthy</span>

<span class="hljs-comment"># Fix it</span>
vim .env  <span class="hljs-comment"># Restore correct DB_PASS</span>
docker compose restart
</div></code></pre>
<p><strong>Method 2: Simulate with manual healthcheck failure</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># SSH into container</span>
docker <span class="hljs-built_in">exec</span> -it passage-plan-app bash

<span class="hljs-comment"># Write ERROR to health file</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">"ERROR <span class="hljs-variable">$(date -Iseconds)</span>"</span> &gt; /app/logs/health_status.txt
<span class="hljs-built_in">echo</span> <span class="hljs-string">"ERROR_MSG: Test error"</span> &gt;&gt; /app/logs/health_status.txt

<span class="hljs-comment"># Exit container</span>
<span class="hljs-built_in">exit</span>

<span class="hljs-comment"># Monitor logs</span>
tail -f /srv/repos/alerts/_docker_health_monitor/logs/monitor.log

<span class="hljs-comment"># Restore</span>
docker <span class="hljs-built_in">exec</span> -it passage-plan-app bash
<span class="hljs-built_in">echo</span> <span class="hljs-string">"OK <span class="hljs-variable">$(date -Iseconds)</span>"</span> &gt; /app/logs/health_status.txt
<span class="hljs-built_in">exit</span>
</div></code></pre>
<h3 id="q-can-i-integrate-with-slackteamspagerduty">Q: Can I integrate with Slack/Teams/PagerDuty?</h3>
<p><strong>A:</strong> Not directly, but you can:</p>
<ol>
<li><strong>Email-to-Slack:</strong> Configure Slack email integration</li>
<li><strong>Email-to-Teams:</strong> Configure Teams email connector</li>
<li><strong>Email-to-PagerDuty:</strong> Use PagerDuty integration email</li>
</ol>
<p><strong>Example (Slack):</strong></p>
<pre class="hljs"><code><div><span class="hljs-comment"># In .env</span>
HEALTH_CHECK_ALERT_EMAILS=ops-team@example.com,alerts-12345@company.slack.com
</div></code></pre>
<p><strong>Alternative:</strong> Modify code to add webhook support (requires Python changes).</p>
<h3 id="q-whats-the-difference-between-script-1-and-script-2">Q: What's the difference between Script 1 and Script 2?</h3>
<p><strong>Script 1</strong> (simple, original):</p>
<ul>
<li>Sequential container checks</li>
<li>One retry, fixed delay</li>
<li>Simpler code (~400 lines)</li>
<li>Good for &lt;10 containers</li>
</ul>
<p><strong>Script 2</strong> (polished, current):</p>
<ul>
<li>Parallel container checks</li>
<li>Multiple retries, exponential backoff</li>
<li>Thread-safe with locks</li>
<li>More sophisticated (~500 lines)</li>
<li>Scales to 50+ containers</li>
<li>Production-grade features</li>
</ul>
<p><strong>Recommendation:</strong> Use Script 2 (what you have). It's backward compatible and ready for growth.</p>
<hr>
<h2 id="monitor-the-monitor-optional">Monitor the Monitor (Optional)</h2>
<h3 id="systemd-watchdog">Systemd Watchdog</h3>
<p>Set up a cron job to ensure the service stays running:</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Edit crontab</span>
crontab -e

<span class="hljs-comment"># Add this line (checks every 5 minutes)</span>
*/5 * * * * systemctl is-active --quiet docker-health-monitor || systemctl start docker-health-monitor
</div></code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Every 5 minutes: Check if service is active</li>
<li>If not active: Start it</li>
<li>Silent if already running</li>
</ul>
<h3 id="external-monitoring">External Monitoring</h3>
<p><strong>Option 1: UptimeRobot</strong></p>
<ul>
<li>Monitor: <code>http://your-server:some-port/health</code> (if you add HTTP endpoint)</li>
<li>Alert: If endpoint doesn't respond</li>
</ul>
<p><strong>Option 2: Healthchecks.io</strong></p>
<ul>
<li>Service pings healthchecks.io every N minutes</li>
<li>Alert: If no ping received</li>
</ul>
<p><strong>Option 3: Dead Man's Switch</strong></p>
<ul>
<li>Service writes heartbeat file every cycle</li>
<li>External script checks file age</li>
<li>Alert: If file too old</li>
</ul>
<hr>
<h2 id="remove-project-from-monitoring">Remove Project from Monitoring</h2>
<h3 id="option-1-remove-healthcheck-recommended">Option 1: Remove Healthcheck (Recommended)</h3>
<p>Stop monitoring without breaking container:</p>
<pre class="hljs"><code><div><span class="hljs-built_in">cd</span> /srv/repos/alerts/project_name

<span class="hljs-comment"># Edit Dockerfile - delete HEALTHCHECK line</span>
vim Dockerfile
<span class="hljs-comment"># Remove the entire HEALTHCHECK --interval=... line</span>

<span class="hljs-comment"># Rebuild</span>
docker compose build --no-cache &amp;&amp; docker compose up -d

<span class="hljs-comment"># Verify - no health status</span>
docker ps | grep project_name
<span class="hljs-comment"># Status shows "Up X hours" (no "healthy" indicator)</span>
</div></code></pre>
<p>Monitor will stop tracking it automatically within 30 seconds.</p>
<h3 id="option-2-stop-container">Option 2: Stop Container</h3>
<pre class="hljs"><code><div>docker compose down
</div></code></pre>
<p>Monitor will send one &quot;not_found&quot; alert, then stop tracking it.</p>
<h3 id="option-3-exclude-from-routing-keep-healthcheck-stop-alerts">Option 3: Exclude from Routing (Keep healthcheck, stop alerts)</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Edit monitor .env</span>
vim /srv/repos/alerts/_docker_health_monitor/.env

<span class="hljs-comment"># Add dummy recipient</span>
CONTAINER_ALERT_ROUTING=project-to-ignore:devnull@localhost

<span class="hljs-comment"># Or remove from routing entirely</span>
<span class="hljs-comment"># Alerts will go to default recipients</span>
</div></code></pre>
<hr>
<h2 id="adding-new-projects">Adding New Projects</h2>
<p><strong>The monitor automatically discovers new projects!</strong></p>
<h3 id="quick-start">Quick Start</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># 1. Deploy new project with healthcheck in Dockerfile</span>
<span class="hljs-built_in">cd</span> /srv/repos/alerts/new_project
docker compose up -d

<span class="hljs-comment"># 2. Monitor discovers it within 30 seconds</span>
<span class="hljs-comment"># No changes to monitor needed!</span>

<span class="hljs-comment"># 3. (Optional) Add project-specific routing</span>
vim /srv/repos/alerts/_docker_health_monitor/.env
<span class="hljs-comment"># Add: CONTAINER_ALERT_ROUTING=...:new-project:team@example.com</span>

<span class="hljs-comment"># Restart monitor</span>
sudo systemctl restart docker-health-monitor
</div></code></pre>
<h3 id="verify-new-project-monitored">Verify New Project Monitored</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Check monitor logs</span>
tail -f /srv/repos/alerts/_docker_health_monitor/logs/monitor.log
<span class="hljs-comment"># Should see: [new-project] new-project-app: unknown → healthy</span>

<span class="hljs-comment"># Check container health</span>
docker ps --format <span class="hljs-string">"table {{.Names}}\t{{.Status}}"</span> | grep new-project
</div></code></pre>
<hr>
<h2 id="uninstallation">Uninstallation</h2>
<pre class="hljs"><code><div><span class="hljs-comment"># Stop and disable service</span>
sudo systemctl stop docker-health-monitor
sudo systemctl <span class="hljs-built_in">disable</span> docker-health-monitor

<span class="hljs-comment"># Remove service file</span>
sudo rm /etc/systemd/system/docker-health-monitor.service

<span class="hljs-comment"># Reload systemd</span>
sudo systemctl daemon-reload

<span class="hljs-comment"># Remove monitoring directory (optional - keeps logs)</span>
rm -rf /srv/repos/alerts/_docker_health_monitor

<span class="hljs-comment"># Remove from crontab if using watchdog</span>
crontab -e
<span class="hljs-comment"># Delete the docker-health-monitor line</span>
</div></code></pre>
<hr>
<h2 id="support--contributing">Support &amp; Contributing</h2>
<h3 id="getting-help">Getting Help</h3>
<ol>
<li><strong>Check this guide</strong> - Most questions answered here</li>
<li><strong>Check logs</strong> - <code>/srv/repos/alerts/_docker_health_monitor/logs/monitor.log</code></li>
<li><strong>Check service status</strong> - <code>sudo systemctl status docker-health-monitor</code></li>
<li><strong>Test manually</strong> - <code>python3 docker_health_monitor.py</code></li>
</ol>
<h3 id="reporting-issues">Reporting Issues</h3>
<p>When reporting issues, include:</p>
<ul>
<li>Monitor logs (last 100 lines)</li>
<li>Service status</li>
<li>Configuration (<code>.env</code> with passwords removed)</li>
<li>Container list (<code>docker ps</code>)</li>
<li>Steps to reproduce</li>
</ul>
<hr>
<h2 id="changelog">Changelog</h2>
<h3 id="version-200-2025-12-11---current">Version 2.0.0 (2025-12-11) - Current</h3>
<ul>
<li><strong>NEW</strong>: Intelligent retry logic with configurable wait times</li>
<li><strong>NEW</strong>: Exponential backoff support for persistent failures</li>
<li><strong>NEW</strong>: Parallel container checks (ThreadPoolExecutor)</li>
<li><strong>NEW</strong>: Thread-safe state management with locks</li>
<li><strong>NEW</strong>: Prevents duplicate retry tasks</li>
<li><strong>NEW</strong>: Configurable retry jitter (prevents thundering herd)</li>
<li><strong>NEW</strong>: Graceful shutdown (waits for pending retries)</li>
<li><strong>IMPROVED</strong>: Scales to 50+ containers with no slowdown</li>
<li><strong>IMPROVED</strong>: ~80% reduction in false positive alerts</li>
<li><strong>IMPROVED</strong>: More detailed logging</li>
<li><strong>IMPROVED</strong>: Better error handling and recovery</li>
</ul>
<h3 id="version-100-2025-11-25">Version 1.0.0 (2025-11-25)</h3>
<ul>
<li>Initial release</li>
<li>Basic container health monitoring</li>
<li>Email alerting with project context</li>
<li>Project-specific alert routing</li>
<li>Systemd service integration</li>
<li>Auto-discovery of containers</li>
<li>Log rotation</li>
</ul>
<hr>
<h2 id="license">License</h2>
<p>[Add your license here]</p>
<hr>
<p><strong>End of Guide</strong></p>
<p>For quick reference, see <a href="README.md">README.md</a></p>

</body>
</html>
